{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "import dgl\n",
    "import dgl.graphbolt as gb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is already preprocessed.\n"
     ]
    }
   ],
   "source": [
    "dataset = gb.BuiltinDataset(\"ogbn-arxiv\",root=\"../../../data/datasets\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset consists of graph, feature and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: node_classification. Number of classes: 40\n"
     ]
    }
   ],
   "source": [
    "graph = dataset.graph\n",
    "feature = dataset.feature\n",
    "train_set = dataset.tasks[0].train_set\n",
    "valid_set = dataset.tasks[0].validation_set\n",
    "test_set = dataset.tasks[0].test_set\n",
    "task_name = dataset.tasks[0].metadata[\"name\"]\n",
    "num_classes = dataset.tasks[0].metadata[\"num_classes\"]\n",
    "print(f\"Task: {task_name}. Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Neighbor Sampler and Data Loader in DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipe = gb.ItemSampler(train_set, batch_size=1024, shuffle=True)\n",
    "datapipe = datapipe.sample_neighbor(graph, [4, 4])\n",
    "datapipe = datapipe.fetch_feature(feature, node_feature_keys=[\"feat\"])\n",
    "datapipe = datapipe.copy_to(device)\n",
    "train_dataloader = gb.DataLoader(datapipe, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over the data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch(seeds=tensor([ 49261,  51944, 111496,  ..., 133230, 136307, 163015]),\n",
      "          sampled_subgraphs=[SampledSubgraphImpl(sampled_csc=CSCFormatBase(indptr=tensor([    0,     4,     8,  ..., 14877, 14881, 14885], dtype=torch.int32),\n",
      "                                                                         indices=tensor([ 1024,  1026,  1025,  ...,  1023, 12086,  3897], dtype=torch.int32),\n",
      "                                                           ),\n",
      "                                               original_row_node_ids=tensor([ 49261,  51944, 111496,  ..., 113926, 165359, 151096]),\n",
      "                                               original_edge_ids=tensor([ 673922, 1328739, 2172331,  ..., 2225270, 2068343,  911667]),\n",
      "                                               original_column_node_ids=tensor([ 49261,  51944, 111496,  ...,  66644, 127174, 166823]),\n",
      "                            ),\n",
      "                            SampledSubgraphImpl(sampled_csc=CSCFormatBase(indptr=tensor([   0,    4,    8,  ..., 3664, 3668, 3672], dtype=torch.int32),\n",
      "                                                                         indices=tensor([1024, 1025, 1026,  ..., 3898, 3899, 1023], dtype=torch.int32),\n",
      "                                                           ),\n",
      "                                               original_row_node_ids=tensor([ 49261,  51944, 111496,  ...,  66644, 127174, 166823]),\n",
      "                                               original_edge_ids=tensor([ 673922, 2172331, 1328739,  ..., 1758234, 2283729, 2478613]),\n",
      "                                               original_column_node_ids=tensor([ 49261,  51944, 111496,  ..., 133230, 136307, 163015]),\n",
      "                            )],\n",
      "          node_features={'feat': tensor([[ 0.0052,  0.2911, -0.3137,  ...,  0.0725, -0.1455, -0.2277],\n",
      "                                [-0.2805,  0.1078, -0.3903,  ...,  0.0562, -0.3462, -0.0785],\n",
      "                                [-0.1151,  0.0973, -0.2473,  ...,  0.1274, -0.0928, -0.2198],\n",
      "                                ...,\n",
      "                                [-0.2927, -0.0705, -0.2954,  ...,  0.0875, -0.0538, -0.2240],\n",
      "                                [-0.1311,  0.0356, -0.3157,  ...,  0.1401, -0.1457, -0.1777],\n",
      "                                [-0.0845, -0.0739, -0.2923,  ...,  0.1349, -0.2195, -0.1625]])},\n",
      "          labels=tensor([20, 24, 28,  ..., 28, 27, 24]),\n",
      "          input_nodes=tensor([ 49261,  51944, 111496,  ..., 113926, 165359, 151096]),\n",
      "          indexes=None,\n",
      "          edge_features=[{},\n",
      "                        {}],\n",
      "          compacted_seeds=None,\n",
      "          blocks=[Block(num_src_nodes=12087, num_dst_nodes=3900, num_edges=14885),\n",
      "                 Block(num_src_nodes=3900, num_dst_nodes=1024, num_edges=3672)],\n",
      "       )\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " node IDs from MFGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input nodes tensor([ 49261,  51944, 111496,  ..., 113926, 165359, 151096]). \n"
     ]
    }
   ],
   "source": [
    "mfgs =data.blocks\n",
    "input_nodes = mfgs[0].srcdata[dgl.NID]\n",
    "print(f\"Input nodes {input_nodes }. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self ,in_feat,h_feat,num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_feat,h_feat,aggregator_type=\"mean\")\n",
    "        self.conv2 = SAGEConv(h_feat,num_classes,aggregator_type=\"mean\")\n",
    "        self.h_feat = h_feat \n",
    "    def forward(self ,mfgs,x):\n",
    "        h= self.conv1(mfgs[0],x)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(mfgs[1],h)\n",
    "        return h \n",
    "in_size = feature.size(\"node\", None, \"feat\")[0]\n",
    "model = myModel(in_size,16,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define valdiation loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipe = gb.ItemSampler(valid_set, batch_size=1024, shuffle=True)\n",
    "datapipe = datapipe.sample_neighbor(graph, [4, 4])\n",
    "datapipe = datapipe.fetch_feature(feature, node_feature_keys=[\"feat\"])\n",
    "datapipe = datapipe.copy_to(device)\n",
    "valid_dataloader = gb.DataLoader(datapipe, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:03, 24.58it/s, loss=1.817, acc=0.511]\n",
      "30it [00:00, 39.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.5140776536125373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step , data in enumerate(tq):\n",
    "            x = data.node_features[\"feat\"]\n",
    "            labels = data.labels\n",
    "\n",
    "            predictions= model(data.blocks,x)\n",
    "\n",
    "            loss = F.cross_entropy(predictions,labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = sklearn.metrics.accuracy_score(\n",
    "                labels.cpu().numpy(),predictions.argmax(1).detach().cpu().numpy()\n",
    "            )\n",
    "            tq.set_postfix(\n",
    "                 {\"loss\": \"%.03f\" % loss.item(), \"acc\": \"%.03f\" % accuracy},\n",
    "                refresh=False,\n",
    "            )\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq:\n",
    "        for step , data in enumerate(tq):\n",
    "            x = data.node_features[\"feat\"]\n",
    "            labels.append(data.labels.cpu().numpy())\n",
    "            predictions.append(model(data.blocks, x).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print(\"Epoch {} Validation Accuracy {}\".format(epoch, accuracy))\n",
    "\n",
    "        # Note that this tutorial do not train the whole model to the end.\n",
    "    \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
